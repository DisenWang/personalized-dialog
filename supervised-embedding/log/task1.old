data/train-task-1.tsv (36144, 2, 113)
data/dev-task-1-500.tsv (500, 2, 113)
data/candidates.tsv (43863, 2, 113)
[INFO] 2017-05-20T06:17:29+0800: Run main with config {'batch_size': 32, 'lr': 0.01, 'save_dir': 'checkpoints/task-1/model', 'epochs': 400, 'negative_cand': 100} (train.py:76)
[INFO] 2017-05-20T06:19:07+0800: Epoch: 0; Train loss: 0.06651068183238588; Dev loss: 0.0052917137145996095; (train.py:98)
[INFO] 2017-05-20T06:22:33+0800: Evaluation: (406, 94, 0.812) (train.py:102)
[DEBUG] 2017-05-20T06:22:33+0800: Saving checkpoint (train.py:105)
[INFO] 2017-05-20T06:24:19+0800: Epoch: 1; Train loss: 0.03725058206251881; Dev loss: 0.005062858104705811; (train.py:98)
[INFO] 2017-05-20T06:26:07+0800: Epoch: 2; Train loss: 0.02616974967486613; Dev loss: 0.0010094109773635864; (train.py:98)
[INFO] 2017-05-20T06:29:33+0800: Evaluation: (392, 108, 0.784) (train.py:102)
[INFO] 2017-05-20T06:31:18+0800: Epoch: 3; Train loss: 0.02130332398211702; Dev loss: 0.0020790596008300783; (train.py:98)
[INFO] 2017-05-20T06:33:04+0800: Epoch: 4; Train loss: 0.021936674278857985; Dev loss: 0.027579612731933593; (train.py:98)
[INFO] 2017-05-20T06:36:36+0800: Evaluation: (410, 90, 0.82) (train.py:102)
[DEBUG] 2017-05-20T06:36:36+0800: Saving checkpoint (train.py:105)
[INFO] 2017-05-20T06:38:19+0800: Epoch: 5; Train loss: 0.017490558231527942; Dev loss: 0.0008107568025588989; (train.py:98)
[INFO] 2017-05-20T06:40:03+0800: Epoch: 6; Train loss: 0.015561256374420261; Dev loss: 0.0030847196578979493; (train.py:98)
[INFO] 2017-05-20T06:43:30+0800: Evaluation: (396, 104, 0.792) (train.py:102)
[INFO] 2017-05-20T06:45:17+0800: Epoch: 7; Train loss: 0.015892570154824273; Dev loss: 0.0016586833000183105; (train.py:98)
[INFO] 2017-05-20T06:47:03+0800: Epoch: 8; Train loss: 0.014138439085622923; Dev loss: 0.00268757963180542; (train.py:98)
[INFO] 2017-05-20T06:50:10+0800: Evaluation: (359, 141, 0.718) (train.py:102)
[INFO] 2017-05-20T06:51:57+0800: Epoch: 9; Train loss: 0.013539822132192365; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T06:53:42+0800: Epoch: 10; Train loss: 0.014023970712307963; Dev loss: 0.002483497142791748; (train.py:98)
[INFO] 2017-05-20T06:56:47+0800: Evaluation: (361, 139, 0.722) (train.py:102)
[INFO] 2017-05-20T06:58:34+0800: Epoch: 11; Train loss: 0.01007285406167562; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T07:00:20+0800: Epoch: 12; Train loss: 0.010365894157610047; Dev loss: 0.011159420967102051; (train.py:98)
[INFO] 2017-05-20T07:03:13+0800: Evaluation: (314, 186, 0.628) (train.py:102)
[INFO] 2017-05-20T07:05:00+0800: Epoch: 13; Train loss: 0.012129197197713451; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T07:06:46+0800: Epoch: 14; Train loss: 0.011265800650916015; Dev loss: 0.004397396087646484; (train.py:98)
[INFO] 2017-05-20T07:09:19+0800: Evaluation: (287, 213, 0.574) (train.py:102)
[INFO] 2017-05-20T07:11:09+0800: Epoch: 15; Train loss: 0.011502618290366041; Dev loss: 0.002562506675720215; (train.py:98)
[INFO] 2017-05-20T07:12:55+0800: Epoch: 16; Train loss: 0.012561596142498743; Dev loss: 0.01712881088256836; (train.py:98)
[INFO] 2017-05-20T07:15:39+0800: Evaluation: (300, 200, 0.6) (train.py:102)
[INFO] 2017-05-20T07:17:26+0800: Epoch: 17; Train loss: 0.011268414533324477; Dev loss: 0.002022457122802734; (train.py:98)
[INFO] 2017-05-20T07:19:16+0800: Epoch: 18; Train loss: 0.010056822226797537; Dev loss: 0.0036857147216796875; (train.py:98)
[INFO] 2017-05-20T07:21:47+0800: Evaluation: (278, 222, 0.556) (train.py:102)
[INFO] 2017-05-20T07:23:34+0800: Epoch: 19; Train loss: 0.012018159230993837; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T07:25:20+0800: Epoch: 20; Train loss: 0.0132373185861918; Dev loss: 0.015040770530700683; (train.py:98)
[INFO] 2017-05-20T07:28:09+0800: Evaluation: (321, 179, 0.642) (train.py:102)
[INFO] 2017-05-20T07:29:54+0800: Epoch: 21; Train loss: 0.010194563698751734; Dev loss: 0.01938759994506836; (train.py:98)
[INFO] 2017-05-20T07:31:41+0800: Epoch: 22; Train loss: 0.010248562703854245; Dev loss: 0.003463634490966797; (train.py:98)
[INFO] 2017-05-20T07:34:15+0800: Evaluation: (283, 217, 0.566) (train.py:102)
[INFO] 2017-05-20T07:36:03+0800: Epoch: 23; Train loss: 0.010713475726319345; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T07:37:53+0800: Epoch: 24; Train loss: 0.008240071319386028; Dev loss: 0.00381832218170166; (train.py:98)
[INFO] 2017-05-20T07:40:28+0800: Evaluation: (291, 209, 0.582) (train.py:102)
[INFO] 2017-05-20T07:42:13+0800: Epoch: 25; Train loss: 0.011271277070605646; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T07:43:59+0800: Epoch: 26; Train loss: 0.01123966956473623; Dev loss: 0.0038495779037475584; (train.py:98)
[INFO] 2017-05-20T07:46:27+0800: Evaluation: (260, 240, 0.52) (train.py:102)
[INFO] 2017-05-20T07:48:12+0800: Epoch: 27; Train loss: 0.010360850107138433; Dev loss: 0.00579918622970581; (train.py:98)
[INFO] 2017-05-20T07:49:59+0800: Epoch: 28; Train loss: 0.011543698676270418; Dev loss: 0.003243438243865967; (train.py:98)
[INFO] 2017-05-20T07:52:20+0800: Evaluation: (259, 241, 0.518) (train.py:102)
[INFO] 2017-05-20T07:54:09+0800: Epoch: 29; Train loss: 0.007218291828405848; Dev loss: 0.002020526885986328; (train.py:98)
[INFO] 2017-05-20T07:55:58+0800: Epoch: 30; Train loss: 0.012021439658018631; Dev loss: 0.0012309762239456176; (train.py:98)
[INFO] 2017-05-20T07:58:09+0800: Evaluation: (220, 280, 0.44) (train.py:102)
[INFO] 2017-05-20T07:59:55+0800: Epoch: 31; Train loss: 0.009951260482129484; Dev loss: 0.005106661796569824; (train.py:98)
[INFO] 2017-05-20T08:01:44+0800: Epoch: 32; Train loss: 0.008888657246577693; Dev loss: 0.00992022705078125; (train.py:98)
[INFO] 2017-05-20T08:03:50+0800: Evaluation: (220, 280, 0.44) (train.py:102)
[INFO] 2017-05-20T08:05:38+0800: Epoch: 33; Train loss: 0.011243952308753514; Dev loss: 0.00023146453499794007; (train.py:98)
[INFO] 2017-05-20T08:07:25+0800: Epoch: 34; Train loss: 0.008814767997000934; Dev loss: 0.001977077603340149; (train.py:98)
[INFO] 2017-05-20T08:09:35+0800: Evaluation: (217, 283, 0.434) (train.py:102)
[INFO] 2017-05-20T08:11:25+0800: Epoch: 35; Train loss: 0.009008919412303995; Dev loss: 0.0004056660830974579; (train.py:98)
[INFO] 2017-05-20T08:13:16+0800: Epoch: 36; Train loss: 0.00945256696329654; Dev loss: 0.0009479133486747742; (train.py:98)
[INFO] 2017-05-20T08:15:19+0800: Evaluation: (214, 286, 0.428) (train.py:102)
[INFO] 2017-05-20T08:17:06+0800: Epoch: 37; Train loss: 0.009276497475006182; Dev loss: 0.004091208934783936; (train.py:98)
[INFO] 2017-05-20T08:18:56+0800: Epoch: 38; Train loss: 0.007974521585405384; Dev loss: 0.001408779263496399; (train.py:98)
[INFO] 2017-05-20T08:20:56+0800: Evaluation: (207, 293, 0.414) (train.py:102)
[INFO] 2017-05-20T08:22:43+0800: Epoch: 39; Train loss: 0.009956392818676814; Dev loss: 0.0004786950945854187; (train.py:98)
[INFO] 2017-05-20T08:24:33+0800: Epoch: 40; Train loss: 0.011810014443647758; Dev loss: 0.010672996520996094; (train.py:98)
[INFO] 2017-05-20T08:26:45+0800: Evaluation: (229, 271, 0.458) (train.py:102)
[INFO] 2017-05-20T08:28:37+0800: Epoch: 41; Train loss: 0.008835213764918198; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T08:30:31+0800: Epoch: 42; Train loss: 0.00695367554062761; Dev loss: 0.0019159585237503052; (train.py:98)
[INFO] 2017-05-20T08:32:35+0800: Evaluation: (215, 285, 0.43) (train.py:102)
[INFO] 2017-05-20T08:34:23+0800: Epoch: 43; Train loss: 0.009977411372526687; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T08:36:12+0800: Epoch: 44; Train loss: 0.008052274080825332; Dev loss: 0.013899178504943847; (train.py:98)
[INFO] 2017-05-20T08:38:07+0800: Evaluation: (185, 315, 0.37) (train.py:102)
[INFO] 2017-05-20T08:40:01+0800: Epoch: 45; Train loss: 0.008652519021960282; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T08:41:48+0800: Epoch: 46; Train loss: 0.011611071111363924; Dev loss: 4.42195068359375; (train.py:98)
[INFO] 2017-05-20T08:43:40+0800: Evaluation: (184, 316, 0.368) (train.py:102)
[INFO] 2017-05-20T08:45:28+0800: Epoch: 47; Train loss: 0.009102652722833808; Dev loss: 0.0008161541819572449; (train.py:98)
[INFO] 2017-05-20T08:47:18+0800: Epoch: 48; Train loss: 0.010047749737402854; Dev loss: 0.006651821136474609; (train.py:98)
[INFO] 2017-05-20T08:49:22+0800: Evaluation: (211, 289, 0.422) (train.py:102)
[INFO] 2017-05-20T08:51:10+0800: Epoch: 49; Train loss: 0.009165694562065542; Dev loss: 0.0012758666276931764; (train.py:98)
[INFO] 2017-05-20T08:52:59+0800: Epoch: 50; Train loss: 0.010232229708705156; Dev loss: 0.002781257152557373; (train.py:98)
[INFO] 2017-05-20T08:54:37+0800: Evaluation: (145, 355, 0.29) (train.py:102)
[INFO] 2017-05-20T08:56:24+0800: Epoch: 51; Train loss: 0.009970785173457224; Dev loss: 0.0025638084411621093; (train.py:98)
[INFO] 2017-05-20T08:58:12+0800: Epoch: 52; Train loss: 0.007937975455536836; Dev loss: 0.0013732305765151978; (train.py:98)
[INFO] 2017-05-20T09:00:08+0800: Evaluation: (196, 304, 0.392) (train.py:102)
[INFO] 2017-05-20T09:02:02+0800: Epoch: 53; Train loss: 0.01237508999157337; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T09:03:53+0800: Epoch: 54; Train loss: 0.007283757623199921; Dev loss: 0.007059970855712891; (train.py:98)
[INFO] 2017-05-20T09:06:02+0800: Evaluation: (220, 280, 0.44) (train.py:102)
[INFO] 2017-05-20T09:07:52+0800: Epoch: 55; Train loss: 0.008037674144324124; Dev loss: 0.000960872769355774; (train.py:98)
[INFO] 2017-05-20T09:09:45+0800: Epoch: 56; Train loss: 0.009063806441835581; Dev loss: 0.0006906506419181824; (train.py:98)
[INFO] 2017-05-20T09:11:32+0800: Evaluation: (177, 323, 0.354) (train.py:102)
[INFO] 2017-05-20T09:13:30+0800: Epoch: 57; Train loss: 0.007944395647614275; Dev loss: 0.00029014983773231507; (train.py:98)
[INFO] 2017-05-20T09:15:15+0800: Epoch: 58; Train loss: 0.013246119928008957; Dev loss: 0.00657748031616211; (train.py:98)
[INFO] 2017-05-20T09:17:16+0800: Evaluation: (205, 295, 0.41) (train.py:102)
[INFO] 2017-05-20T09:19:05+0800: Epoch: 59; Train loss: 0.006071921810017171; Dev loss: 0.009512117385864258; (train.py:98)
[INFO] 2017-05-20T09:20:56+0800: Epoch: 60; Train loss: 0.009089390668631422; Dev loss: 0.001653347134590149; (train.py:98)
[INFO] 2017-05-20T09:22:47+0800: Evaluation: (175, 325, 0.35) (train.py:102)
[INFO] 2017-05-20T09:24:34+0800: Epoch: 61; Train loss: 0.009166771899655233; Dev loss: 0.0004220697283744812; (train.py:98)
[INFO] 2017-05-20T09:26:22+0800: Epoch: 62; Train loss: 0.007061594927619777; Dev loss: 0.0007999268770217896; (train.py:98)
[INFO] 2017-05-20T09:28:14+0800: Evaluation: (186, 314, 0.372) (train.py:102)
[INFO] 2017-05-20T09:30:06+0800: Epoch: 63; Train loss: 0.008114794781157396; Dev loss: 0.0003779065012931824; (train.py:98)
[INFO] 2017-05-20T09:31:53+0800: Epoch: 64; Train loss: 0.01183033014521662; Dev loss: 0.0009026614427566529; (train.py:98)
[INFO] 2017-05-20T09:33:37+0800: Evaluation: (176, 324, 0.352) (train.py:102)
[INFO] 2017-05-20T09:35:23+0800: Epoch: 65; Train loss: 0.009427642964489178; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T09:37:13+0800: Epoch: 66; Train loss: 0.009029338375561973; Dev loss: 0.002013503551483154; (train.py:98)
[INFO] 2017-05-20T09:39:16+0800: Evaluation: (203, 297, 0.406) (train.py:102)
[INFO] 2017-05-20T09:41:06+0800: Epoch: 67; Train loss: 0.008478389024729756; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T09:42:58+0800: Epoch: 68; Train loss: 0.00917711093246468; Dev loss: 0.003288530349731445; (train.py:98)
[INFO] 2017-05-20T09:44:31+0800: Evaluation: (157, 343, 0.314) (train.py:102)
[INFO] 2017-05-20T09:46:25+0800: Epoch: 69; Train loss: 0.00868323329879161; Dev loss: 0.0025614047050476073; (train.py:98)
[INFO] 2017-05-20T09:48:14+0800: Epoch: 70; Train loss: 0.010122295735954176; Dev loss: 0.0002969419062137604; (train.py:98)
[INFO] 2017-05-20T09:50:17+0800: Evaluation: (212, 288, 0.424) (train.py:102)
[INFO] 2017-05-20T09:52:09+0800: Epoch: 71; Train loss: 0.007090350026278186; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T09:54:04+0800: Epoch: 72; Train loss: 0.008386929559374196; Dev loss: 0.001986475944519043; (train.py:98)
[INFO] 2017-05-20T09:55:51+0800: Evaluation: (168, 332, 0.336) (train.py:102)
[INFO] 2017-05-20T09:57:44+0800: Epoch: 73; Train loss: 0.007058501518068129; Dev loss: 0.0019346685409545898; (train.py:98)
[INFO] 2017-05-20T09:59:34+0800: Epoch: 74; Train loss: 0.009945544064107926; Dev loss: 0.0014059444665908813; (train.py:98)
[INFO] 2017-05-20T10:01:22+0800: Evaluation: (172, 328, 0.344) (train.py:102)
[INFO] 2017-05-20T10:03:11+0800: Epoch: 75; Train loss: 0.010462015681031634; Dev loss: 0.00028725829720497133; (train.py:98)
[INFO] 2017-05-20T10:05:01+0800: Epoch: 76; Train loss: 0.007749888074507313; Dev loss: 0.0018678966760635376; (train.py:98)
[INFO] 2017-05-20T10:06:45+0800: Evaluation: (161, 339, 0.322) (train.py:102)
[INFO] 2017-05-20T10:08:37+0800: Epoch: 77; Train loss: 0.006602180786767868; Dev loss: 0.0019046050310134888; (train.py:98)
[INFO] 2017-05-20T10:10:30+0800: Epoch: 78; Train loss: 0.009165119174069092; Dev loss: 0.004083827018737793; (train.py:98)
[INFO] 2017-05-20T10:12:32+0800: Evaluation: (203, 297, 0.406) (train.py:102)
[INFO] 2017-05-20T10:14:28+0800: Epoch: 79; Train loss: 0.006325538399795338; Dev loss: 0.00035654085874557495; (train.py:98)
[INFO] 2017-05-20T10:16:22+0800: Epoch: 80; Train loss: 0.007482521661508094; Dev loss: 0.0026667704582214354; (train.py:98)
[INFO] 2017-05-20T10:18:17+0800: Evaluation: (194, 306, 0.388) (train.py:102)
[INFO] 2017-05-20T10:20:09+0800: Epoch: 81; Train loss: 0.007679184592724644; Dev loss: 0.0002244683802127838; (train.py:98)
[INFO] 2017-05-20T10:21:59+0800: Epoch: 82; Train loss: 0.009455996205747946; Dev loss: 0.003082812786102295; (train.py:98)
[INFO] 2017-05-20T10:23:48+0800: Evaluation: (176, 324, 0.352) (train.py:102)
[INFO] 2017-05-20T10:25:43+0800: Epoch: 83; Train loss: 0.007620787393742238; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T10:27:32+0800: Epoch: 84; Train loss: 0.010124408050152486; Dev loss: 0.003812629699707031; (train.py:98)
[INFO] 2017-05-20T10:29:11+0800: Evaluation: (159, 341, 0.318) (train.py:102)
[INFO] 2017-05-20T10:31:06+0800: Epoch: 85; Train loss: 0.007353593867129183; Dev loss: 0.011355867385864258; (train.py:98)
[INFO] 2017-05-20T10:32:58+0800: Epoch: 86; Train loss: 0.008906293582970907; Dev loss: 0.0015797648429870606; (train.py:98)
[INFO] 2017-05-20T10:34:40+0800: Evaluation: (164, 336, 0.328) (train.py:102)
[INFO] 2017-05-20T10:36:33+0800: Epoch: 87; Train loss: 0.008559147608806056; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T10:38:27+0800: Epoch: 88; Train loss: 0.007814719961006784; Dev loss: 0.008250836372375489; (train.py:98)
[INFO] 2017-05-20T10:39:44+0800: Evaluation: (93, 407, 0.186) (train.py:102)
[INFO] 2017-05-20T10:41:37+0800: Epoch: 89; Train loss: 0.006893931281543244; Dev loss: 0.0012963024377822877; (train.py:98)
[INFO] 2017-05-20T10:43:27+0800: Epoch: 90; Train loss: 0.011407047992112196; Dev loss: 0.0029169654846191405; (train.py:98)
[INFO] 2017-05-20T10:45:18+0800: Evaluation: (192, 308, 0.384) (train.py:102)
[INFO] 2017-05-20T10:47:05+0800: Epoch: 91; Train loss: 0.008100866169409311; Dev loss: 0.00036055392026901247; (train.py:98)
[INFO] 2017-05-20T10:48:57+0800: Epoch: 92; Train loss: 0.008279089546737051; Dev loss: 0.00731829023361206; (train.py:98)
[INFO] 2017-05-20T10:50:18+0800: Evaluation: (109, 391, 0.218) (train.py:102)
[INFO] 2017-05-20T10:52:05+0800: Epoch: 93; Train loss: 0.010097665865955572; Dev loss: 0.00035679978132247925; (train.py:98)
[INFO] 2017-05-20T10:53:53+0800: Epoch: 94; Train loss: 0.0092515995334738; Dev loss: 0.0012595216035842895; (train.py:98)
[INFO] 2017-05-20T10:55:13+0800: Evaluation: (111, 389, 0.222) (train.py:102)
[INFO] 2017-05-20T10:57:04+0800: Epoch: 95; Train loss: 0.008186433396613223; Dev loss: 0.0022723917961120607; (train.py:98)
[INFO] 2017-05-20T10:58:52+0800: Epoch: 96; Train loss: 0.009871358621084049; Dev loss: 0.006104667186737061; (train.py:98)
[INFO] 2017-05-20T10:59:59+0800: Evaluation: (79, 421, 0.158) (train.py:102)
[INFO] 2017-05-20T11:01:45+0800: Epoch: 97; Train loss: 0.0062863802863027275; Dev loss: 0.0004086572527885437; (train.py:98)
[INFO] 2017-05-20T11:03:38+0800: Epoch: 98; Train loss: 0.00958216140873888; Dev loss: 0.0010818310976028443; (train.py:98)
[INFO] 2017-05-20T11:04:54+0800: Evaluation: (98, 402, 0.196) (train.py:102)
[INFO] 2017-05-20T11:06:45+0800: Epoch: 99; Train loss: 0.009400836146079398; Dev loss: 0.001161785125732422; (train.py:98)
[INFO] 2017-05-20T11:08:33+0800: Epoch: 100; Train loss: 0.009743466584037474; Dev loss: 0.0013470258712768555; (train.py:98)
[INFO] 2017-05-20T11:10:14+0800: Evaluation: (155, 345, 0.31) (train.py:102)
[INFO] 2017-05-20T11:12:02+0800: Epoch: 101; Train loss: 0.007841208785199368; Dev loss: 0.010445796012878417; (train.py:98)
[INFO] 2017-05-20T11:13:54+0800: Epoch: 102; Train loss: 0.006566348510930185; Dev loss: 0.0013651500940322877; (train.py:98)
[INFO] 2017-05-20T11:15:07+0800: Evaluation: (94, 406, 0.188) (train.py:102)
[INFO] 2017-05-20T11:16:57+0800: Epoch: 103; Train loss: 0.008211024094798349; Dev loss: 0.00018160644173622131; (train.py:98)
[INFO] 2017-05-20T11:18:44+0800: Epoch: 104; Train loss: 0.010097664932020738; Dev loss: 0.0015812908411026; (train.py:98)
[INFO] 2017-05-20T11:20:16+0800: Evaluation: (141, 359, 0.282) (train.py:102)
[INFO] 2017-05-20T11:22:07+0800: Epoch: 105; Train loss: 0.007856118175066717; Dev loss: 0.0003272930979728699; (train.py:98)
[INFO] 2017-05-20T11:24:00+0800: Epoch: 106; Train loss: 0.00662953514090472; Dev loss: 0.02140570831298828; (train.py:98)
[INFO] 2017-05-20T11:25:22+0800: Evaluation: (126, 374, 0.252) (train.py:102)
[INFO] 2017-05-20T11:27:14+0800: Epoch: 107; Train loss: 0.010257294615534814; Dev loss: 0.0021924400329589844; (train.py:98)
[INFO] 2017-05-20T11:29:04+0800: Epoch: 108; Train loss: 0.009260811920803627; Dev loss: 0.0021079416275024414; (train.py:98)
[INFO] 2017-05-20T11:30:00+0800: Evaluation: (65, 435, 0.13) (train.py:102)
[INFO] 2017-05-20T11:31:50+0800: Epoch: 109; Train loss: 0.007367129411912515; Dev loss: 0.004196429252624511; (train.py:98)
[INFO] 2017-05-20T11:33:41+0800: Epoch: 110; Train loss: 0.008684286646364153; Dev loss: 0.0034703330993652343; (train.py:98)
[INFO] 2017-05-20T11:35:10+0800: Evaluation: (125, 375, 0.25) (train.py:102)
[INFO] 2017-05-20T11:37:02+0800: Epoch: 111; Train loss: 0.007609408386176367; Dev loss: 0.0007343407869338989; (train.py:98)
[INFO] 2017-05-20T11:38:59+0800: Epoch: 112; Train loss: 0.007297894797152244; Dev loss: 0.0006187555193901062; (train.py:98)
[INFO] 2017-05-20T11:40:43+0800: Evaluation: (163, 337, 0.326) (train.py:102)
[INFO] 2017-05-20T11:42:33+0800: Epoch: 113; Train loss: 0.009117376192712714; Dev loss: 0.0014094616174697876; (train.py:98)
[INFO] 2017-05-20T11:44:21+0800: Epoch: 114; Train loss: 0.011163550324366157; Dev loss: 0.0026703758239746095; (train.py:98)
[INFO] 2017-05-20T11:45:45+0800: Evaluation: (123, 377, 0.246) (train.py:102)
[INFO] 2017-05-20T11:47:38+0800: Epoch: 115; Train loss: 0.007850337184254132; Dev loss: 0.004835352897644043; (train.py:98)
[INFO] 2017-05-20T11:49:29+0800: Epoch: 116; Train loss: 0.008786242753749646; Dev loss: 0.020520891189575195; (train.py:98)
[INFO] 2017-05-20T11:50:56+0800: Evaluation: (121, 379, 0.242) (train.py:102)
[INFO] 2017-05-20T11:52:51+0800: Epoch: 117; Train loss: 0.006152189020323062; Dev loss: 0.006460170745849609; (train.py:98)
[INFO] 2017-05-20T11:54:40+0800: Epoch: 118; Train loss: 0.014018750947810653; Dev loss: 0.0053131175041198735; (train.py:98)
[INFO] 2017-05-20T11:56:22+0800: Evaluation: (157, 343, 0.314) (train.py:102)
[INFO] 2017-05-20T11:58:11+0800: Epoch: 119; Train loss: 0.008317330279613252; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T11:59:57+0800: Epoch: 120; Train loss: 0.007680483385915371; Dev loss: 0.009050061225891113; (train.py:98)
[INFO] 2017-05-20T12:01:03+0800: Evaluation: (74, 426, 0.148) (train.py:102)
[INFO] 2017-05-20T12:02:57+0800: Epoch: 121; Train loss: 0.008326833473586825; Dev loss: 0.00101016628742218; (train.py:98)
[INFO] 2017-05-20T12:04:51+0800: Epoch: 122; Train loss: 0.008226403731910178; Dev loss: 0.011140450477600098; (train.py:98)
[INFO] 2017-05-20T12:06:32+0800: Evaluation: (159, 341, 0.318) (train.py:102)
[INFO] 2017-05-20T12:08:31+0800: Epoch: 123; Train loss: 0.007911134182451596; Dev loss: 0.0002934075891971588; (train.py:98)
[INFO] 2017-05-20T12:10:29+0800: Epoch: 124; Train loss: 0.00956029788068722; Dev loss: 0.002093462944030762; (train.py:98)
[INFO] 2017-05-20T12:12:11+0800: Evaluation: (159, 341, 0.318) (train.py:102)
[INFO] 2017-05-20T12:14:02+0800: Epoch: 125; Train loss: 0.007811910119089605; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T12:16:00+0800: Epoch: 126; Train loss: 0.008435356511958907; Dev loss: 0.008824450492858887; (train.py:98)
[INFO] 2017-05-20T12:17:39+0800: Evaluation: (149, 351, 0.298) (train.py:102)
[INFO] 2017-05-20T12:19:31+0800: Epoch: 127; Train loss: 0.011203978248340959; Dev loss: 0.001718207836151123; (train.py:98)
[INFO] 2017-05-20T12:21:24+0800: Epoch: 128; Train loss: 0.0071660195937381845; Dev loss: 0.0029812536239624023; (train.py:98)
[INFO] 2017-05-20T12:23:02+0800: Evaluation: (148, 352, 0.296) (train.py:102)
[INFO] 2017-05-20T12:24:54+0800: Epoch: 129; Train loss: 0.009213769648990632; Dev loss: 0.000994106411933899; (train.py:98)
[INFO] 2017-05-20T12:26:45+0800: Epoch: 130; Train loss: 0.008850184381308591; Dev loss: 0.004902904033660889; (train.py:98)
[INFO] 2017-05-20T12:28:32+0800: Evaluation: (179, 321, 0.358) (train.py:102)
[INFO] 2017-05-20T12:30:27+0800: Epoch: 131; Train loss: 0.011326324777088004; Dev loss: 0.0010668102502822876; (train.py:98)
[INFO] 2017-05-20T12:32:21+0800: Epoch: 132; Train loss: 0.007061779807105696; Dev loss: 0.0055754237174987795; (train.py:98)
[INFO] 2017-05-20T12:33:49+0800: Evaluation: (136, 364, 0.272) (train.py:102)
[INFO] 2017-05-20T12:35:42+0800: Epoch: 133; Train loss: 0.009026034498154777; Dev loss: 0.0014848899841308594; (train.py:98)
[INFO] 2017-05-20T12:37:32+0800: Epoch: 134; Train loss: 0.007367998450764547; Dev loss: 0.022900825500488282; (train.py:98)
[INFO] 2017-05-20T12:39:03+0800: Evaluation: (131, 369, 0.262) (train.py:102)
[INFO] 2017-05-20T12:40:57+0800: Epoch: 135; Train loss: 0.006354322760560599; Dev loss: 0.0008305932283401489; (train.py:98)
[INFO] 2017-05-20T12:42:52+0800: Epoch: 136; Train loss: 0.007450135967732564; Dev loss: 0.004648444652557373; (train.py:98)
[INFO] 2017-05-20T12:44:18+0800: Evaluation: (125, 375, 0.25) (train.py:102)
[INFO] 2017-05-20T12:46:12+0800: Epoch: 137; Train loss: 0.00752051463250523; Dev loss: 0.0008491479158401489; (train.py:98)
[INFO] 2017-05-20T12:48:09+0800: Epoch: 138; Train loss: 0.011300355643892538; Dev loss: 0.007296608448028564; (train.py:98)
[INFO] 2017-05-20T12:49:38+0800: Evaluation: (140, 360, 0.28) (train.py:102)
[INFO] 2017-05-20T12:51:29+0800: Epoch: 139; Train loss: 0.007647771207482157; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T12:53:19+0800: Epoch: 140; Train loss: 0.008141713984944083; Dev loss: 0.0023182029724121093; (train.py:98)
[INFO] 2017-05-20T12:55:02+0800: Evaluation: (160, 340, 0.32) (train.py:102)
[INFO] 2017-05-20T12:56:53+0800: Epoch: 141; Train loss: 0.008827838503497238; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T12:58:46+0800: Epoch: 142; Train loss: 0.008764681739190691; Dev loss: 0.008432868957519532; (train.py:98)
[INFO] 2017-05-20T13:00:29+0800: Evaluation: (156, 344, 0.312) (train.py:102)
[INFO] 2017-05-20T13:02:22+0800: Epoch: 143; Train loss: 0.00916620434134621; Dev loss: 0.002120398998260498; (train.py:98)
[INFO] 2017-05-20T13:04:16+0800: Epoch: 144; Train loss: 0.0067477974985419445; Dev loss: 0.003935024261474609; (train.py:98)
[INFO] 2017-05-20T13:05:40+0800: Evaluation: (127, 373, 0.254) (train.py:102)
[INFO] 2017-05-20T13:07:29+0800: Epoch: 145; Train loss: 0.007175419017567512; Dev loss: 0.00027044311165809633; (train.py:98)
[INFO] 2017-05-20T13:09:24+0800: Epoch: 146; Train loss: 0.006988881798756885; Dev loss: 0.016964225769042968; (train.py:98)
[INFO] 2017-05-20T13:10:52+0800: Evaluation: (137, 363, 0.274) (train.py:102)
[INFO] 2017-05-20T13:12:53+0800: Epoch: 147; Train loss: 0.0071549689213265205; Dev loss: 0.0002823869287967682; (train.py:98)
[INFO] 2017-05-20T13:14:49+0800: Epoch: 148; Train loss: 0.008148488754013057; Dev loss: 0.002747699737548828; (train.py:98)
[INFO] 2017-05-20T13:16:05+0800: Evaluation: (124, 376, 0.248) (train.py:102)
[INFO] 2017-05-20T13:17:53+0800: Epoch: 149; Train loss: 0.009037295806544473; Dev loss: 0.002583126068115234; (train.py:98)
[INFO] 2017-05-20T13:19:51+0800: Epoch: 150; Train loss: 0.006633301965260302; Dev loss: 0.04228549194335938; (train.py:98)
[INFO] 2017-05-20T13:21:13+0800: Evaluation: (117, 383, 0.234) (train.py:102)
[INFO] 2017-05-20T13:23:04+0800: Epoch: 151; Train loss: 0.00993752325958949; Dev loss: 0.0012577892541885377; (train.py:98)
[INFO] 2017-05-20T13:24:54+0800: Epoch: 152; Train loss: 0.011117512068616707; Dev loss: 0.0025180320739746093; (train.py:98)
[INFO] 2017-05-20T13:26:23+0800: Evaluation: (127, 373, 0.254) (train.py:102)
[INFO] 2017-05-20T13:28:16+0800: Epoch: 153; Train loss: 0.007398748629036017; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T13:30:11+0800: Epoch: 154; Train loss: 0.009638976417114462; Dev loss: 0.0020537683963775637; (train.py:98)
[INFO] 2017-05-20T13:31:50+0800: Evaluation: (155, 345, 0.31) (train.py:102)
[INFO] 2017-05-20T13:33:40+0800: Epoch: 155; Train loss: 0.007663246706992243; Dev loss: 0.0004795495867729187; (train.py:98)
[INFO] 2017-05-20T13:35:30+0800: Epoch: 156; Train loss: 0.007547241825259556; Dev loss: 0.012881457328796387; (train.py:98)
[INFO] 2017-05-20T13:36:54+0800: Evaluation: (108, 392, 0.216) (train.py:102)
[INFO] 2017-05-20T13:38:47+0800: Epoch: 157; Train loss: 0.0071058789284992796; Dev loss: 0.006151454925537109; (train.py:98)
[INFO] 2017-05-20T13:40:43+0800: Epoch: 158; Train loss: 0.007769613659336888; Dev loss: 0.0025657148361206057; (train.py:98)
[INFO] 2017-05-20T13:41:57+0800: Evaluation: (95, 405, 0.19) (train.py:102)
[INFO] 2017-05-20T13:43:54+0800: Epoch: 159; Train loss: 0.007045324080055369; Dev loss: 0.00023102965950965883; (train.py:98)
[INFO] 2017-05-20T13:45:48+0800: Epoch: 160; Train loss: 0.006617033670142795; Dev loss: 0.0023345603942871093; (train.py:98)
[INFO] 2017-05-20T13:47:09+0800: Evaluation: (113, 387, 0.226) (train.py:102)
[INFO] 2017-05-20T13:48:57+0800: Epoch: 161; Train loss: 0.012258508526460375; Dev loss: 0.0003161737024784088; (train.py:98)
[INFO] 2017-05-20T13:50:52+0800: Epoch: 162; Train loss: 0.008461047735018531; Dev loss: 0.0010189892053604126; (train.py:98)
[INFO] 2017-05-20T13:52:20+0800: Evaluation: (132, 368, 0.264) (train.py:102)
[INFO] 2017-05-20T13:54:16+0800: Epoch: 163; Train loss: 0.009100294094912535; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T13:56:09+0800: Epoch: 164; Train loss: 0.006534035467703391; Dev loss: 0.0038288450241088868; (train.py:98)
[INFO] 2017-05-20T13:57:53+0800: Evaluation: (164, 336, 0.328) (train.py:102)
[INFO] 2017-05-20T13:59:48+0800: Epoch: 165; Train loss: 0.006503180870055042; Dev loss: 0.0029721140861511232; (train.py:98)
[INFO] 2017-05-20T14:01:40+0800: Epoch: 166; Train loss: 0.01026679809991076; Dev loss: 0.001265285611152649; (train.py:98)
[INFO] 2017-05-20T14:03:06+0800: Evaluation: (120, 380, 0.24) (train.py:102)
[INFO] 2017-05-20T14:05:01+0800: Epoch: 167; Train loss: 0.008500650695397512; Dev loss: 0.00031954589486122133; (train.py:98)
[INFO] 2017-05-20T14:06:58+0800: Epoch: 168; Train loss: 0.009348886912065979; Dev loss: 0.00893123722076416; (train.py:98)
[INFO] 2017-05-20T14:08:32+0800: Evaluation: (139, 361, 0.278) (train.py:102)
[INFO] 2017-05-20T14:10:26+0800: Epoch: 169; Train loss: 0.010243911723011688; Dev loss: 0.00047024506330490114; (train.py:98)
[INFO] 2017-05-20T14:12:19+0800: Epoch: 170; Train loss: 0.010463883106864436; Dev loss: 0.00039017456769943235; (train.py:98)
[INFO] 2017-05-20T14:13:31+0800: Evaluation: (92, 408, 0.184) (train.py:102)
[INFO] 2017-05-20T14:15:19+0800: Epoch: 171; Train loss: 0.007617030380884925; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T14:17:13+0800: Epoch: 172; Train loss: 0.0074353478334428956; Dev loss: 0.013201247215270996; (train.py:98)
[INFO] 2017-05-20T14:18:50+0800: Evaluation: (157, 343, 0.314) (train.py:102)
[INFO] 2017-05-20T14:20:42+0800: Epoch: 173; Train loss: 0.007411715423595738; Dev loss: 0.001800971508026123; (train.py:98)
[INFO] 2017-05-20T14:22:40+0800: Epoch: 174; Train loss: 0.008188452681943143; Dev loss: 0.013582043647766114; (train.py:98)
[INFO] 2017-05-20T14:23:27+0800: Evaluation: (49, 451, 0.098) (train.py:102)
[INFO] 2017-05-20T14:25:20+0800: Epoch: 175; Train loss: 0.009471757302631; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T14:27:17+0800: Epoch: 176; Train loss: 0.007394622590689355; Dev loss: 0.001122728943824768; (train.py:98)
[INFO] 2017-05-20T14:28:22+0800: Evaluation: (81, 419, 0.162) (train.py:102)
[INFO] 2017-05-20T14:30:21+0800: Epoch: 177; Train loss: 0.006586970856480549; Dev loss: 0.0012820014953613282; (train.py:98)
[INFO] 2017-05-20T14:32:14+0800: Epoch: 178; Train loss: 0.007568019034787056; Dev loss: 0.003576728343963623; (train.py:98)
[INFO] 2017-05-20T14:33:20+0800: Evaluation: (71, 429, 0.142) (train.py:102)
[INFO] 2017-05-20T14:35:12+0800: Epoch: 179; Train loss: 0.008794457782554119; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T14:37:08+0800: Epoch: 180; Train loss: 0.010491813792135; Dev loss: 0.001624416470527649; (train.py:98)
[INFO] 2017-05-20T14:38:47+0800: Evaluation: (152, 348, 0.304) (train.py:102)
[INFO] 2017-05-20T14:40:42+0800: Epoch: 181; Train loss: 0.009404044541564427; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T14:42:38+0800: Epoch: 182; Train loss: 0.0072233471734641595; Dev loss: 0.0004988977313041687; (train.py:98)
[INFO] 2017-05-20T14:43:53+0800: Evaluation: (103, 397, 0.206) (train.py:102)
[INFO] 2017-05-20T14:45:55+0800: Epoch: 183; Train loss: 0.005350754168358701; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T14:47:51+0800: Epoch: 184; Train loss: 0.008487529078303788; Dev loss: 0.0023548240661621094; (train.py:98)
[INFO] 2017-05-20T14:48:59+0800: Evaluation: (88, 412, 0.176) (train.py:102)
[INFO] 2017-05-20T14:50:58+0800: Epoch: 185; Train loss: 0.006409812784861858; Dev loss: 0.007829424858093261; (train.py:98)
[INFO] 2017-05-20T14:52:56+0800: Epoch: 186; Train loss: 0.006900508265076358; Dev loss: 0.002422817230224609; (train.py:98)
[INFO] 2017-05-20T14:54:26+0800: Evaluation: (132, 368, 0.264) (train.py:102)
[INFO] 2017-05-20T14:56:26+0800: Epoch: 187; Train loss: 0.006502986848934612; Dev loss: 0.0003570215106010437; (train.py:98)
[INFO] 2017-05-20T14:58:30+0800: Epoch: 188; Train loss: 0.009638065069859444; Dev loss: 0.0019266015291213988; (train.py:98)
[INFO] 2017-05-20T14:59:34+0800: Evaluation: (83, 417, 0.166) (train.py:102)
[INFO] 2017-05-20T15:01:33+0800: Epoch: 189; Train loss: 0.010000378332428977; Dev loss: 0.00017011229693889617; (train.py:98)
[INFO] 2017-05-20T15:03:27+0800: Epoch: 190; Train loss: 0.009430448587405327; Dev loss: 0.004480597972869873; (train.py:98)
[INFO] 2017-05-20T15:05:09+0800: Evaluation: (154, 346, 0.308) (train.py:102)
[INFO] 2017-05-20T15:07:08+0800: Epoch: 191; Train loss: 0.007250372900855025; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T15:09:01+0800: Epoch: 192; Train loss: 0.009738479851135524; Dev loss: 0.003625414848327637; (train.py:98)
[INFO] 2017-05-20T15:10:47+0800: Evaluation: (161, 339, 0.322) (train.py:102)
[INFO] 2017-05-20T15:12:41+0800: Epoch: 193; Train loss: 0.00805089538501066; Dev loss: 0.003398688793182373; (train.py:98)
[INFO] 2017-05-20T15:14:40+0800: Epoch: 194; Train loss: 0.00817617626050984; Dev loss: 0.002770881175994873; (train.py:98)
[INFO] 2017-05-20T15:15:45+0800: Evaluation: (87, 413, 0.174) (train.py:102)
[INFO] 2017-05-20T15:17:37+0800: Epoch: 195; Train loss: 0.008073320103083226; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T15:19:30+0800: Epoch: 196; Train loss: 0.0089735120456694; Dev loss: 0.0026661067008972167; (train.py:98)
[INFO] 2017-05-20T15:21:10+0800: Evaluation: (142, 358, 0.284) (train.py:102)
[INFO] 2017-05-20T15:23:01+0800: Epoch: 197; Train loss: 0.007934523545697845; Dev loss: 0.0002999536097049713; (train.py:98)
[INFO] 2017-05-20T15:24:54+0800: Epoch: 198; Train loss: 0.0073947981265743545; Dev loss: 0.0017985302209854127; (train.py:98)
[INFO] 2017-05-20T15:26:13+0800: Evaluation: (101, 399, 0.202) (train.py:102)
[INFO] 2017-05-20T15:28:08+0800: Epoch: 199; Train loss: 0.009716374196053717; Dev loss: 0.00430680799484253; (train.py:98)
[INFO] 2017-05-20T15:29:54+0800: Epoch: 200; Train loss: 0.011133478419772164; Dev loss: 0.010707131385803223; (train.py:98)
[INFO] 2017-05-20T15:31:19+0800: Evaluation: (116, 384, 0.232) (train.py:102)
[INFO] 2017-05-20T15:33:15+0800: Epoch: 201; Train loss: 0.007942326538801324; Dev loss: 0.0007300988435745239; (train.py:98)
[INFO] 2017-05-20T15:35:04+0800: Epoch: 202; Train loss: 0.011186140805405873; Dev loss: 0.0018918725252151489; (train.py:98)
[INFO] 2017-05-20T15:36:05+0800: Evaluation: (76, 424, 0.152) (train.py:102)
[INFO] 2017-05-20T15:38:02+0800: Epoch: 203; Train loss: 0.006625425637549803; Dev loss: 0.0021449241638183594; (train.py:98)
[INFO] 2017-05-20T15:39:54+0800: Epoch: 204; Train loss: 0.007468470449858494; Dev loss: 0.0024578123092651367; (train.py:98)
[INFO] 2017-05-20T15:40:56+0800: Evaluation: (78, 422, 0.156) (train.py:102)
[INFO] 2017-05-20T15:42:53+0800: Epoch: 205; Train loss: 0.013027097247991873; Dev loss: 0.015633319854736327; (train.py:98)
[INFO] 2017-05-20T15:44:48+0800: Epoch: 206; Train loss: 0.009039270659467418; Dev loss: 0.006936103343963623; (train.py:98)
[INFO] 2017-05-20T15:46:27+0800: Evaluation: (154, 346, 0.308) (train.py:102)
[INFO] 2017-05-20T15:48:17+0800: Epoch: 207; Train loss: 0.016667600299696855; Dev loss: 0.0013698692321777344; (train.py:98)
[INFO] 2017-05-20T15:50:07+0800: Epoch: 208; Train loss: 0.007729104677050928; Dev loss: 0.003848090648651123; (train.py:98)
[INFO] 2017-05-20T15:51:43+0800: Evaluation: (152, 348, 0.304) (train.py:102)
[INFO] 2017-05-20T15:53:36+0800: Epoch: 209; Train loss: 0.008647556879609513; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T15:55:28+0800: Epoch: 210; Train loss: 0.010178781149953941; Dev loss: 0.009405583381652831; (train.py:98)
[INFO] 2017-05-20T15:56:28+0800: Evaluation: (70, 430, 0.14) (train.py:102)
[INFO] 2017-05-20T15:58:14+0800: Epoch: 211; Train loss: 0.011753669732746339; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T16:00:09+0800: Epoch: 212; Train loss: 0.006699501508635074; Dev loss: 0.003526130199432373; (train.py:98)
[INFO] 2017-05-20T16:01:13+0800: Evaluation: (77, 423, 0.154) (train.py:102)
[INFO] 2017-05-20T16:03:08+0800: Epoch: 213; Train loss: 0.011728978144624277; Dev loss: 0.0005151636004447937; (train.py:98)
[INFO] 2017-05-20T16:05:04+0800: Epoch: 214; Train loss: 0.007536197532790426; Dev loss: 0.01595481204986572; (train.py:98)
[INFO] 2017-05-20T16:06:38+0800: Evaluation: (154, 346, 0.308) (train.py:102)
[INFO] 2017-05-20T16:08:38+0800: Epoch: 215; Train loss: 0.006037113274325072; Dev loss: 0.0011267772912979127; (train.py:98)
[INFO] 2017-05-20T16:10:29+0800: Epoch: 216; Train loss: 0.00881441144859245; Dev loss: 0.011525988578796387; (train.py:98)
[INFO] 2017-05-20T16:12:00+0800: Evaluation: (135, 365, 0.27) (train.py:102)
[INFO] 2017-05-20T16:13:56+0800: Epoch: 217; Train loss: 0.00943077386552771; Dev loss: 0.0021433525085449217; (train.py:98)
[INFO] 2017-05-20T16:15:56+0800: Epoch: 218; Train loss: 0.0072721670589880995; Dev loss: 0.01898102569580078; (train.py:98)
[INFO] 2017-05-20T16:17:28+0800: Evaluation: (140, 360, 0.28) (train.py:102)
[INFO] 2017-05-20T16:19:25+0800: Epoch: 219; Train loss: 0.008299800245091938; Dev loss: 0.0007114526033401489; (train.py:98)
[INFO] 2017-05-20T16:21:19+0800: Epoch: 220; Train loss: 0.010226650237923735; Dev loss: 0.002063514232635498; (train.py:98)
[INFO] 2017-05-20T16:22:35+0800: Evaluation: (107, 393, 0.214) (train.py:102)
[INFO] 2017-05-20T16:24:30+0800: Epoch: 221; Train loss: 0.008913724689933853; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T16:26:33+0800: Epoch: 222; Train loss: 0.006160398207944185; Dev loss: 0.006142665863037109; (train.py:98)
[INFO] 2017-05-20T16:27:58+0800: Evaluation: (131, 369, 0.262) (train.py:102)
[INFO] 2017-05-20T16:29:51+0800: Epoch: 223; Train loss: 0.012459578957660564; Dev loss: 0.0010439635515213013; (train.py:98)
[INFO] 2017-05-20T16:31:46+0800: Epoch: 224; Train loss: 0.008479136201984799; Dev loss: 0.007381659984588623; (train.py:98)
[INFO] 2017-05-20T16:33:02+0800: Evaluation: (102, 398, 0.204) (train.py:102)
[INFO] 2017-05-20T16:35:00+0800: Epoch: 225; Train loss: 0.008289971739084544; Dev loss: 0.0043564410209655765; (train.py:98)
[INFO] 2017-05-20T16:37:00+0800: Epoch: 226; Train loss: 0.008236956463136429; Dev loss: 0.0009773827791213989; (train.py:98)
[INFO] 2017-05-20T16:38:33+0800: Evaluation: (148, 352, 0.296) (train.py:102)
[INFO] 2017-05-20T16:40:41+0800: Epoch: 227; Train loss: 0.0053622503655464; Dev loss: 0.0035191307067871094; (train.py:98)
[INFO] 2017-05-20T16:42:38+0800: Epoch: 228; Train loss: 0.010153142168472516; Dev loss: 0.008006030082702637; (train.py:98)
[INFO] 2017-05-20T16:44:10+0800: Evaluation: (134, 366, 0.268) (train.py:102)
[INFO] 2017-05-20T16:46:06+0800: Epoch: 229; Train loss: 0.00946834257285013; Dev loss: 0.0029383201599121095; (train.py:98)
[INFO] 2017-05-20T16:48:04+0800: Epoch: 230; Train loss: 0.007653410294291701; Dev loss: 0.009840241432189941; (train.py:98)
[INFO] 2017-05-20T16:49:29+0800: Evaluation: (135, 365, 0.27) (train.py:102)
[INFO] 2017-05-20T16:51:23+0800: Epoch: 231; Train loss: 0.008230974787148746; Dev loss: 0.0014345359802246094; (train.py:98)
[INFO] 2017-05-20T16:53:12+0800: Epoch: 232; Train loss: 0.011237130524805405; Dev loss: 0.0029971580505371094; (train.py:98)
[INFO] 2017-05-20T16:54:43+0800: Evaluation: (141, 359, 0.282) (train.py:102)
[INFO] 2017-05-20T16:56:44+0800: Epoch: 233; Train loss: 0.005719407850917884; Dev loss: 0.00043164795637130735; (train.py:98)
[INFO] 2017-05-20T16:58:38+0800: Epoch: 234; Train loss: 0.007226528370878777; Dev loss: 0.001722957730293274; (train.py:98)
[INFO] 2017-05-20T17:00:07+0800: Evaluation: (134, 366, 0.268) (train.py:102)
[INFO] 2017-05-20T17:02:00+0800: Epoch: 235; Train loss: 0.008693462711178226; Dev loss: 0.0015199047327041626; (train.py:98)
[INFO] 2017-05-20T17:03:54+0800: Epoch: 236; Train loss: 0.014077522217361202; Dev loss: 0.000993435025215149; (train.py:98)
[INFO] 2017-05-20T17:05:18+0800: Evaluation: (120, 380, 0.24) (train.py:102)
[INFO] 2017-05-20T17:07:09+0800: Epoch: 237; Train loss: 0.009199246068830566; Dev loss: 0.0021523094177246095; (train.py:98)
[INFO] 2017-05-20T17:09:00+0800: Epoch: 238; Train loss: 0.00957126712907201; Dev loss: 0.002516608715057373; (train.py:98)
[INFO] 2017-05-20T17:10:40+0800: Evaluation: (152, 348, 0.304) (train.py:102)
[INFO] 2017-05-20T17:12:35+0800: Epoch: 239; Train loss: 0.011744830717869439; Dev loss: 0.0017849998474121095; (train.py:98)
[INFO] 2017-05-20T17:14:29+0800: Epoch: 240; Train loss: 0.009180693635762899; Dev loss: 0.001866787075996399; (train.py:98)
[INFO] 2017-05-20T17:15:58+0800: Evaluation: (144, 356, 0.288) (train.py:102)
[INFO] 2017-05-20T17:17:53+0800: Epoch: 241; Train loss: 0.0077841291761229035; Dev loss: 0.001722358226776123; (train.py:98)
[INFO] 2017-05-20T17:19:52+0800: Epoch: 242; Train loss: 0.008253831865847062; Dev loss: 0.008704068183898926; (train.py:98)
[INFO] 2017-05-20T17:21:04+0800: Evaluation: (98, 402, 0.196) (train.py:102)
[INFO] 2017-05-20T17:22:59+0800: Epoch: 243; Train loss: 0.008918002416879527; Dev loss: 0.0005237390398979187; (train.py:98)
[INFO] 2017-05-20T17:24:51+0800: Epoch: 244; Train loss: 0.014792807513483911; Dev loss: 0.0037047996520996095; (train.py:98)
[INFO] 2017-05-20T17:26:22+0800: Evaluation: (137, 363, 0.274) (train.py:102)
[INFO] 2017-05-20T17:28:14+0800: Epoch: 245; Train loss: 0.009886142227347468; Dev loss: 0.0002995263636112213; (train.py:98)
[INFO] 2017-05-20T17:30:12+0800: Epoch: 246; Train loss: 0.009933194250929787; Dev loss: 0.011127285957336426; (train.py:98)
[INFO] 2017-05-20T17:31:28+0800: Evaluation: (110, 390, 0.22) (train.py:102)
[INFO] 2017-05-20T17:33:28+0800: Epoch: 247; Train loss: 0.009281907477400122; Dev loss: 0.002641089916229248; (train.py:98)
[INFO] 2017-05-20T17:35:21+0800: Epoch: 248; Train loss: 0.008929030587027325; Dev loss: 0.001909725308418274; (train.py:98)
[INFO] 2017-05-20T17:36:54+0800: Evaluation: (141, 359, 0.282) (train.py:102)
[INFO] 2017-05-20T17:38:52+0800: Epoch: 249; Train loss: 0.00990237726320893; Dev loss: 0.001708167552947998; (train.py:98)
[INFO] 2017-05-20T17:40:48+0800: Epoch: 250; Train loss: 0.007211527184809172; Dev loss: 0.0023107142448425292; (train.py:98)
[INFO] 2017-05-20T17:42:13+0800: Evaluation: (132, 368, 0.264) (train.py:102)
[INFO] 2017-05-20T17:44:04+0800: Epoch: 251; Train loss: 0.011881298459554586; Dev loss: 0.0015102152824401855; (train.py:98)
[INFO] 2017-05-20T17:45:57+0800: Epoch: 252; Train loss: 0.008412829569562356; Dev loss: 0.007733974456787109; (train.py:98)
[INFO] 2017-05-20T17:47:21+0800: Evaluation: (127, 373, 0.254) (train.py:102)
[INFO] 2017-05-20T17:49:22+0800: Epoch: 253; Train loss: 0.008763341730329216; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T17:51:17+0800: Epoch: 254; Train loss: 0.00673969214151291; Dev loss: 0.002409247875213623; (train.py:98)
[INFO] 2017-05-20T17:52:48+0800: Evaluation: (138, 362, 0.276) (train.py:102)
[INFO] 2017-05-20T17:54:49+0800: Epoch: 255; Train loss: 0.010594628978396022; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T17:56:46+0800: Epoch: 256; Train loss: 0.007782816518832063; Dev loss: 0.013518868446350097; (train.py:98)
[INFO] 2017-05-20T17:58:09+0800: Evaluation: (117, 383, 0.234) (train.py:102)
[INFO] 2017-05-20T18:00:08+0800: Epoch: 257; Train loss: 0.00811523087206709; Dev loss: 0.0009047509431838989; (train.py:98)
[INFO] 2017-05-20T18:02:00+0800: Epoch: 258; Train loss: 0.010514608374815876; Dev loss: 0.0010148693323135376; (train.py:98)
[INFO] 2017-05-20T18:03:47+0800: Evaluation: (172, 328, 0.344) (train.py:102)
[INFO] 2017-05-20T18:05:46+0800: Epoch: 259; Train loss: 0.008651817465083419; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T18:07:47+0800: Epoch: 260; Train loss: 0.007314570520619229; Dev loss: 0.01226216983795166; (train.py:98)
[INFO] 2017-05-20T18:09:21+0800: Evaluation: (143, 357, 0.286) (train.py:102)
[INFO] 2017-05-20T18:11:24+0800: Epoch: 261; Train loss: 0.0070828496339175795; Dev loss: 0.0004047510027885437; (train.py:98)
[INFO] 2017-05-20T18:13:21+0800: Epoch: 262; Train loss: 0.006635200216266586; Dev loss: 0.0015764657258987426; (train.py:98)
[INFO] 2017-05-20T18:14:52+0800: Evaluation: (152, 348, 0.304) (train.py:102)
[INFO] 2017-05-20T18:16:46+0800: Epoch: 263; Train loss: 0.011011305752921694; Dev loss: 0.0005079004168510437; (train.py:98)
[INFO] 2017-05-20T18:18:40+0800: Epoch: 264; Train loss: 0.006532235663568588; Dev loss: 0.002199592113494873; (train.py:98)
[INFO] 2017-05-20T18:20:03+0800: Evaluation: (109, 391, 0.218) (train.py:102)
[INFO] 2017-05-20T18:22:02+0800: Epoch: 265; Train loss: 0.00847611182032414; Dev loss: 0.014322884559631347; (train.py:98)
[INFO] 2017-05-20T18:24:03+0800: Epoch: 266; Train loss: 0.007853836244149048; Dev loss: 0.004739814281463623; (train.py:98)
[INFO] 2017-05-20T18:25:15+0800: Evaluation: (114, 386, 0.228) (train.py:102)
[INFO] 2017-05-20T18:27:21+0800: Epoch: 267; Train loss: 0.007805683841921007; Dev loss: 0.0006916577219963074; (train.py:98)
[INFO] 2017-05-20T18:29:13+0800: Epoch: 268; Train loss: 0.010479719887895533; Dev loss: 0.000880840003490448; (train.py:98)
[INFO] 2017-05-20T18:30:36+0800: Evaluation: (129, 371, 0.258) (train.py:102)
[INFO] 2017-05-20T18:32:32+0800: Epoch: 269; Train loss: 0.008722432951400948; Dev loss: 0.0003308984637260437; (train.py:98)
[INFO] 2017-05-20T18:34:30+0800: Epoch: 270; Train loss: 0.007191701823443128; Dev loss: 0.0055703349113464355; (train.py:98)
[INFO] 2017-05-20T18:36:03+0800: Evaluation: (136, 364, 0.272) (train.py:102)
[INFO] 2017-05-20T18:38:05+0800: Epoch: 271; Train loss: 0.009062830991967263; Dev loss: 0.00039031189680099486; (train.py:98)
[INFO] 2017-05-20T18:39:54+0800: Epoch: 272; Train loss: 0.01091481118988266; Dev loss: 0.001833217740058899; (train.py:98)
[INFO] 2017-05-20T18:41:29+0800: Evaluation: (153, 347, 0.306) (train.py:102)
[INFO] 2017-05-20T18:43:24+0800: Epoch: 273; Train loss: 0.009168637923120932; Dev loss: 0.0003414575457572937; (train.py:98)
[INFO] 2017-05-20T18:45:20+0800: Epoch: 274; Train loss: 0.007658637645657987; Dev loss: 0.0031226463317871093; (train.py:98)
[INFO] 2017-05-20T18:46:30+0800: Evaluation: (96, 404, 0.192) (train.py:102)
[INFO] 2017-05-20T18:48:32+0800: Epoch: 275; Train loss: 0.010717534125839649; Dev loss: 0.0010006176233291627; (train.py:98)
[INFO] 2017-05-20T18:50:28+0800: Epoch: 276; Train loss: 0.006496921347895528; Dev loss: 0.001197200894355774; (train.py:98)
[INFO] 2017-05-20T18:52:03+0800: Evaluation: (161, 339, 0.322) (train.py:102)
[INFO] 2017-05-20T18:54:01+0800: Epoch: 277; Train loss: 0.011009820543517965; Dev loss: 0.0004268457293510437; (train.py:98)
[INFO] 2017-05-20T18:55:56+0800: Epoch: 278; Train loss: 0.011398052711343523; Dev loss: 0.0016026989221572877; (train.py:98)
[INFO] 2017-05-20T18:57:32+0800: Evaluation: (138, 362, 0.276) (train.py:102)
[INFO] 2017-05-20T18:59:35+0800: Epoch: 279; Train loss: 0.009699112002459742; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T19:01:31+0800: Epoch: 280; Train loss: 0.010952954912295787; Dev loss: 0.001861599087715149; (train.py:98)
[INFO] 2017-05-20T19:02:51+0800: Evaluation: (111, 389, 0.222) (train.py:102)
[INFO] 2017-05-20T19:04:45+0800: Epoch: 281; Train loss: 0.008982448636265282; Dev loss: 0.0006312524080276489; (train.py:98)
[INFO] 2017-05-20T19:06:38+0800: Epoch: 282; Train loss: 0.013754918294172464; Dev loss: 0.001978705883026123; (train.py:98)
[INFO] 2017-05-20T19:08:05+0800: Evaluation: (141, 359, 0.282) (train.py:102)
[INFO] 2017-05-20T19:10:03+0800: Epoch: 283; Train loss: 0.013434760079786843; Dev loss: 0.005425136566162109; (train.py:98)
[INFO] 2017-05-20T19:12:00+0800: Epoch: 284; Train loss: 0.00768140490545824; Dev loss: 0.0016016918420791626; (train.py:98)
[INFO] 2017-05-20T19:13:38+0800: Evaluation: (159, 341, 0.318) (train.py:102)
[INFO] 2017-05-20T19:15:33+0800: Epoch: 285; Train loss: 0.008253476496434623; Dev loss: 0.0004498559832572937; (train.py:98)
[INFO] 2017-05-20T19:17:25+0800: Epoch: 286; Train loss: 0.0089782299353993; Dev loss: 0.0009185363650321961; (train.py:98)
[INFO] 2017-05-20T19:19:05+0800: Evaluation: (153, 347, 0.306) (train.py:102)
[INFO] 2017-05-20T19:21:05+0800: Epoch: 287; Train loss: 0.009411864297517262; Dev loss: 0.003476630687713623; (train.py:98)
[INFO] 2017-05-20T19:23:00+0800: Epoch: 288; Train loss: 0.009025840823399047; Dev loss: 0.001328498363494873; (train.py:98)
[INFO] 2017-05-20T19:24:37+0800: Evaluation: (151, 349, 0.302) (train.py:102)
[INFO] 2017-05-20T19:26:26+0800: Epoch: 289; Train loss: 0.00995952681182379; Dev loss: 0.0007375146150588989; (train.py:98)
[INFO] 2017-05-20T19:28:28+0800: Epoch: 290; Train loss: 0.006318509448669074; Dev loss: 0.0012846556901931763; (train.py:98)
[INFO] 2017-05-20T19:30:07+0800: Evaluation: (160, 340, 0.32) (train.py:102)
[INFO] 2017-05-20T19:32:06+0800: Epoch: 291; Train loss: 0.01117768996862481; Dev loss: 0.00027289548516273496; (train.py:98)
[INFO] 2017-05-20T19:34:01+0800: Epoch: 292; Train loss: 0.009173127848942236; Dev loss: 0.0024387888908386232; (train.py:98)
[INFO] 2017-05-20T19:35:40+0800: Evaluation: (161, 339, 0.322) (train.py:102)
[INFO] 2017-05-20T19:37:34+0800: Epoch: 293; Train loss: 0.01864375057313719; Dev loss: 0.0008639336824417114; (train.py:98)
[INFO] 2017-05-20T19:39:31+0800: Epoch: 294; Train loss: 0.009170101354862948; Dev loss: 0.0038406639099121094; (train.py:98)
[INFO] 2017-05-20T19:41:12+0800: Evaluation: (153, 347, 0.306) (train.py:102)
[INFO] 2017-05-20T19:43:10+0800: Epoch: 295; Train loss: 0.00830654536941148; Dev loss: 0.00605086898803711; (train.py:98)
[INFO] 2017-05-20T19:45:03+0800: Epoch: 296; Train loss: 0.010795102358494537; Dev loss: 0.0012704735994338988; (train.py:98)
[INFO] 2017-05-20T19:46:26+0800: Evaluation: (124, 376, 0.248) (train.py:102)
[INFO] 2017-05-20T19:48:16+0800: Epoch: 297; Train loss: 0.01062938613176425; Dev loss: 0.0027401118278503416; (train.py:98)
[INFO] 2017-05-20T19:50:15+0800: Epoch: 298; Train loss: 0.005855010085350732; Dev loss: 0.04488507843017578; (train.py:98)
[INFO] 2017-05-20T19:51:27+0800: Evaluation: (107, 393, 0.214) (train.py:102)
[INFO] 2017-05-20T19:53:26+0800: Epoch: 299; Train loss: 0.007736317336841908; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T19:55:30+0800: Epoch: 300; Train loss: 0.007556083047209656; Dev loss: 0.0022333641052246092; (train.py:98)
[INFO] 2017-05-20T19:57:14+0800: Evaluation: (167, 333, 0.334) (train.py:102)
[INFO] 2017-05-20T19:59:08+0800: Epoch: 301; Train loss: 0.010561949404384471; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T20:01:00+0800: Epoch: 302; Train loss: 0.008465771009563623; Dev loss: 0.002216773509979248; (train.py:98)
[INFO] 2017-05-20T20:02:31+0800: Evaluation: (140, 360, 0.28) (train.py:102)
[INFO] 2017-05-20T20:04:29+0800: Epoch: 303; Train loss: 0.006692848555796156; Dev loss: 0.005075019359588623; (train.py:98)
[INFO] 2017-05-20T20:06:23+0800: Epoch: 304; Train loss: 0.010258395923727764; Dev loss: 0.007792324066162109; (train.py:98)
[INFO] 2017-05-20T20:07:49+0800: Evaluation: (133, 367, 0.266) (train.py:102)
[INFO] 2017-05-20T20:09:44+0800: Epoch: 305; Train loss: 0.006338354578109676; Dev loss: 0.00402962875366211; (train.py:98)
[INFO] 2017-05-20T20:11:37+0800: Epoch: 306; Train loss: 0.012716741161285922; Dev loss: 0.003221503734588623; (train.py:98)
[INFO] 2017-05-20T20:12:55+0800: Evaluation: (108, 392, 0.216) (train.py:102)
[INFO] 2017-05-20T20:14:52+0800: Epoch: 307; Train loss: 0.00834394091731696; Dev loss: 0.0005179101824760437; (train.py:98)
[INFO] 2017-05-20T20:16:49+0800: Epoch: 308; Train loss: 0.015804966515764567; Dev loss: 0.004275356292724609; (train.py:98)
[INFO] 2017-05-20T20:18:12+0800: Evaluation: (113, 387, 0.226) (train.py:102)
[INFO] 2017-05-20T20:20:13+0800: Epoch: 309; Train loss: 0.008448334872413834; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T20:22:13+0800: Epoch: 310; Train loss: 0.006550419392928851; Dev loss: 0.002272834300994873; (train.py:98)
[INFO] 2017-05-20T20:23:29+0800: Evaluation: (115, 385, 0.23) (train.py:102)
[INFO] 2017-05-20T20:25:32+0800: Epoch: 311; Train loss: 0.011126205028378376; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T20:27:29+0800: Epoch: 312; Train loss: 0.00900612358980012; Dev loss: 0.027091865539550782; (train.py:98)
[INFO] 2017-05-20T20:29:02+0800: Evaluation: (138, 362, 0.276) (train.py:102)
[INFO] 2017-05-20T20:30:53+0800: Epoch: 313; Train loss: 0.012832189859600178; Dev loss: 0.00662252426147461; (train.py:98)
[INFO] 2017-05-20T20:32:48+0800: Epoch: 314; Train loss: 0.00976025995583976; Dev loss: 0.0026303367614746095; (train.py:98)
[INFO] 2017-05-20T20:34:22+0800: Evaluation: (156, 344, 0.312) (train.py:102)
[INFO] 2017-05-20T20:36:14+0800: Epoch: 315; Train loss: 0.008386017678324266; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T20:38:03+0800: Epoch: 316; Train loss: 0.01063567901231217; Dev loss: 0.0012360912561416625; (train.py:98)
[INFO] 2017-05-20T20:39:41+0800: Evaluation: (152, 348, 0.304) (train.py:102)
[INFO] 2017-05-20T20:41:32+0800: Epoch: 317; Train loss: 0.011721651320914224; Dev loss: 0.002009253978729248; (train.py:98)
[INFO] 2017-05-20T20:43:26+0800: Epoch: 318; Train loss: 0.007951130499327292; Dev loss: 0.01961699104309082; (train.py:98)
[INFO] 2017-05-20T20:44:55+0800: Evaluation: (147, 353, 0.294) (train.py:102)
[INFO] 2017-05-20T20:46:52+0800: Epoch: 319; Train loss: 0.008586695392970339; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T20:48:46+0800: Epoch: 320; Train loss: 0.01083117519814526; Dev loss: 0.004182990550994873; (train.py:98)
[INFO] 2017-05-20T20:50:10+0800: Evaluation: (118, 382, 0.236) (train.py:102)
[INFO] 2017-05-20T20:52:12+0800: Epoch: 321; Train loss: 0.00808182147931701; Dev loss: 0.0007193871736526489; (train.py:98)
[INFO] 2017-05-20T20:54:05+0800: Epoch: 322; Train loss: 0.011297312044504442; Dev loss: 0.0021531639099121092; (train.py:98)
[INFO] 2017-05-20T20:55:35+0800: Evaluation: (130, 370, 0.26) (train.py:102)
[INFO] 2017-05-20T20:57:33+0800: Epoch: 323; Train loss: 0.010387304645190828; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T20:59:31+0800: Epoch: 324; Train loss: 0.007543994931524435; Dev loss: 0.001942836880683899; (train.py:98)
[INFO] 2017-05-20T21:01:04+0800: Evaluation: (134, 366, 0.268) (train.py:102)
[INFO] 2017-05-20T21:02:59+0800: Epoch: 325; Train loss: 0.009338625740823565; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T21:04:57+0800: Epoch: 326; Train loss: 0.008646519445204958; Dev loss: 0.0024917869567871094; (train.py:98)
[INFO] 2017-05-20T21:06:20+0800: Evaluation: (115, 385, 0.23) (train.py:102)
[INFO] 2017-05-20T21:08:14+0800: Epoch: 327; Train loss: 0.010795108521822633; Dev loss: 0.0023893308639526367; (train.py:98)
[INFO] 2017-05-20T21:10:13+0800: Epoch: 328; Train loss: 0.007180205403802703; Dev loss: 0.007946520805358888; (train.py:98)
[INFO] 2017-05-20T21:11:32+0800: Evaluation: (109, 391, 0.218) (train.py:102)
[INFO] 2017-05-20T21:13:35+0800: Epoch: 329; Train loss: 0.006403089341798331; Dev loss: 0.001047786831855774; (train.py:98)
[INFO] 2017-05-20T21:15:30+0800: Epoch: 330; Train loss: 0.0075390820953887385; Dev loss: 0.0024355125427246095; (train.py:98)
[INFO] 2017-05-20T21:17:11+0800: Evaluation: (158, 342, 0.316) (train.py:102)
[INFO] 2017-05-20T21:19:08+0800: Epoch: 331; Train loss: 0.010023877232907227; Dev loss: 0.00028402343392372134; (train.py:98)
[INFO] 2017-05-20T21:21:07+0800: Epoch: 332; Train loss: 0.009785290938655088; Dev loss: 0.0005591699481010437; (train.py:98)
[INFO] 2017-05-20T21:22:30+0800: Evaluation: (127, 373, 0.254) (train.py:102)
[INFO] 2017-05-20T21:24:26+0800: Epoch: 333; Train loss: 0.008289183087487412; Dev loss: 0.0007994652986526489; (train.py:98)
[INFO] 2017-05-20T21:26:29+0800: Epoch: 334; Train loss: 0.007281761498632831; Dev loss: 0.0011856957674026489; (train.py:98)
[INFO] 2017-05-20T21:28:02+0800: Evaluation: (150, 350, 0.3) (train.py:102)
[INFO] 2017-05-20T21:29:58+0800: Epoch: 335; Train loss: 0.015079706417385945; Dev loss: 0.0016091575622558594; (train.py:98)
[INFO] 2017-05-20T21:31:50+0800: Epoch: 336; Train loss: 0.011555907323184716; Dev loss: 0.0007697021365165711; (train.py:98)
[INFO] 2017-05-20T21:33:17+0800: Evaluation: (132, 368, 0.264) (train.py:102)
[INFO] 2017-05-20T21:35:11+0800: Epoch: 337; Train loss: 0.013457924410190199; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T21:37:04+0800: Epoch: 338; Train loss: 0.0077073606723344805; Dev loss: 0.001072231411933899; (train.py:98)
[INFO] 2017-05-20T21:38:38+0800: Evaluation: (147, 353, 0.294) (train.py:102)
[INFO] 2017-05-20T21:40:34+0800: Epoch: 339; Train loss: 0.009552477766600351; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T21:42:32+0800: Epoch: 340; Train loss: 0.011765576938487034; Dev loss: 0.0033734006881713866; (train.py:98)
[INFO] 2017-05-20T21:44:16+0800: Evaluation: (157, 343, 0.314) (train.py:102)
[INFO] 2017-05-20T21:46:19+0800: Epoch: 341; Train loss: 0.006142297762126801; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T21:48:13+0800: Epoch: 342; Train loss: 0.01646008105013998; Dev loss: 0.0023006358146667482; (train.py:98)
[INFO] 2017-05-20T21:49:37+0800: Evaluation: (124, 376, 0.248) (train.py:102)
[INFO] 2017-05-20T21:51:29+0800: Epoch: 343; Train loss: 0.007714997251593723; Dev loss: 0.0031326560974121094; (train.py:98)
[INFO] 2017-05-20T21:53:28+0800: Epoch: 344; Train loss: 0.010458002515609145; Dev loss: 0.014593106269836426; (train.py:98)
[INFO] 2017-05-20T21:54:55+0800: Evaluation: (136, 364, 0.272) (train.py:102)
[INFO] 2017-05-20T21:56:49+0800: Epoch: 345; Train loss: 0.008410843681683619; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T21:58:44+0800: Epoch: 346; Train loss: 0.01084225655250864; Dev loss: 0.000807216763496399; (train.py:98)
[INFO] 2017-05-20T22:00:14+0800: Evaluation: (118, 382, 0.236) (train.py:102)
[INFO] 2017-05-20T22:02:15+0800: Epoch: 347; Train loss: 0.007019017625270488; Dev loss: 0.0003838159441947937; (train.py:98)
[INFO] 2017-05-20T22:04:08+0800: Epoch: 348; Train loss: 0.01292081165711491; Dev loss: 0.02055746078491211; (train.py:98)
[INFO] 2017-05-20T22:05:36+0800: Evaluation: (132, 368, 0.264) (train.py:102)
[INFO] 2017-05-20T22:07:33+0800: Epoch: 349; Train loss: 0.006603068843292837; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T22:09:38+0800: Epoch: 350; Train loss: 0.006106185086371213; Dev loss: 0.004946403026580811; (train.py:98)
[INFO] 2017-05-20T22:11:11+0800: Evaluation: (148, 352, 0.296) (train.py:102)
[INFO] 2017-05-20T22:13:11+0800: Epoch: 351; Train loss: 0.009963520457301782; Dev loss: 0.0010760155916213988; (train.py:98)
[INFO] 2017-05-20T22:15:05+0800: Epoch: 352; Train loss: 0.011161776494892684; Dev loss: 0.0024993662834167482; (train.py:98)
[INFO] 2017-05-20T22:16:43+0800: Evaluation: (144, 356, 0.288) (train.py:102)
[INFO] 2017-05-20T22:18:35+0800: Epoch: 353; Train loss: 0.010050273039813172; Dev loss: 0.00021285644173622131; (train.py:98)
[INFO] 2017-05-20T22:20:33+0800: Epoch: 354; Train loss: 0.014011858896897683; Dev loss: 0.004031806468963623; (train.py:98)
[INFO] 2017-05-20T22:22:13+0800: Evaluation: (151, 349, 0.302) (train.py:102)
[INFO] 2017-05-20T22:24:05+0800: Epoch: 355; Train loss: 0.00793758735964186; Dev loss: 0.0020268015861511232; (train.py:98)
[INFO] 2017-05-20T22:26:05+0800: Epoch: 356; Train loss: 0.006591345295856281; Dev loss: 0.006130805492401123; (train.py:98)
[INFO] 2017-05-20T22:27:36+0800: Evaluation: (138, 362, 0.276) (train.py:102)
[INFO] 2017-05-20T22:29:35+0800: Epoch: 357; Train loss: 0.010391064859365392; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T22:31:33+0800: Epoch: 358; Train loss: 0.009449926874945731; Dev loss: 0.004111746788024903; (train.py:98)
[INFO] 2017-05-20T22:33:12+0800: Evaluation: (165, 335, 0.33) (train.py:102)
[INFO] 2017-05-20T22:35:07+0800: Epoch: 359; Train loss: 0.011054965739105752; Dev loss: 0.0014255332946777343; (train.py:98)
[INFO] 2017-05-20T22:36:58+0800: Epoch: 360; Train loss: 0.013252713249858399; Dev loss: 0.014803902626037597; (train.py:98)
[INFO] 2017-05-20T22:38:39+0800: Evaluation: (161, 339, 0.322) (train.py:102)
[INFO] 2017-05-20T22:40:34+0800: Epoch: 361; Train loss: 0.008295221649951971; Dev loss: 0.0007476464509963989; (train.py:98)
[INFO] 2017-05-20T22:42:42+0800: Epoch: 362; Train loss: 0.00721306885207732; Dev loss: 0.0021543455123901366; (train.py:98)
[INFO] 2017-05-20T22:44:18+0800: Evaluation: (140, 360, 0.28) (train.py:102)
[INFO] 2017-05-20T22:46:20+0800: Epoch: 363; Train loss: 0.008422196193432993; Dev loss: 0.0003793298602104187; (train.py:98)
[INFO] 2017-05-20T22:48:25+0800: Epoch: 364; Train loss: 0.01171957796366144; Dev loss: 0.03371469116210937; (train.py:98)
[INFO] 2017-05-20T22:50:06+0800: Evaluation: (153, 347, 0.306) (train.py:102)
[INFO] 2017-05-20T22:52:00+0800: Epoch: 365; Train loss: 0.008467361255922104; Dev loss: 0.001834235668182373; (train.py:98)
[INFO] 2017-05-20T22:53:53+0800: Epoch: 366; Train loss: 0.013578355356574928; Dev loss: 0.005276537895202636; (train.py:98)
[INFO] 2017-05-20T22:55:31+0800: Evaluation: (148, 352, 0.296) (train.py:102)
[INFO] 2017-05-20T22:57:29+0800: Epoch: 367; Train loss: 0.00923887650808103; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T22:59:29+0800: Epoch: 368; Train loss: 0.007633245459469361; Dev loss: 0.0018674999475479126; (train.py:98)
[INFO] 2017-05-20T23:00:55+0800: Evaluation: (129, 371, 0.258) (train.py:102)
[INFO] 2017-05-20T23:02:49+0800: Epoch: 369; Train loss: 0.008901805889579596; Dev loss: 0.0013562889099121094; (train.py:98)
[INFO] 2017-05-20T23:04:41+0800: Epoch: 370; Train loss: 0.015005722919973945; Dev loss: 0.0012309228181838988; (train.py:98)
[INFO] 2017-05-20T23:06:03+0800: Evaluation: (113, 387, 0.226) (train.py:102)
[INFO] 2017-05-20T23:08:05+0800: Epoch: 371; Train loss: 0.009099066752989682; Dev loss: 0.005939582347869873; (train.py:98)
[INFO] 2017-05-20T23:10:03+0800: Epoch: 372; Train loss: 0.015229212515087158; Dev loss: 0.00437874984741211; (train.py:98)
[INFO] 2017-05-20T23:11:21+0800: Evaluation: (108, 392, 0.216) (train.py:102)
[INFO] 2017-05-20T23:13:25+0800: Epoch: 373; Train loss: 0.01253724328069088; Dev loss: 0.0002825581133365631; (train.py:98)
[INFO] 2017-05-20T23:15:21+0800: Epoch: 374; Train loss: 0.007834305519057951; Dev loss: 0.003980597972869873; (train.py:98)
[INFO] 2017-05-20T23:16:46+0800: Evaluation: (124, 376, 0.248) (train.py:102)
[INFO] 2017-05-20T23:18:49+0800: Epoch: 375; Train loss: 0.006943497408299751; Dev loss: 0.0005411798357963562; (train.py:98)
[INFO] 2017-05-20T23:20:48+0800: Epoch: 376; Train loss: 0.01488401908861754; Dev loss: 0.0073189158439636234; (train.py:98)
[INFO] 2017-05-20T23:22:19+0800: Evaluation: (133, 367, 0.266) (train.py:102)
[INFO] 2017-05-20T23:24:18+0800: Epoch: 377; Train loss: 0.014434906700714139; Dev loss: 0.000786281704902649; (train.py:98)
[INFO] 2017-05-20T23:26:11+0800: Epoch: 378; Train loss: 0.011278280826996032; Dev loss: 0.011751004219055176; (train.py:98)
[INFO] 2017-05-20T23:27:48+0800: Evaluation: (146, 354, 0.292) (train.py:102)
[INFO] 2017-05-20T23:29:48+0800: Epoch: 379; Train loss: 0.008933631302802768; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T23:31:47+0800: Epoch: 380; Train loss: 0.00806725096937849; Dev loss: 0.0008009911775588989; (train.py:98)
[INFO] 2017-05-20T23:33:26+0800: Evaluation: (144, 356, 0.288) (train.py:102)
[INFO] 2017-05-20T23:35:23+0800: Epoch: 381; Train loss: 0.007920208899690261; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T23:37:28+0800: Epoch: 382; Train loss: 0.0069490826546050434; Dev loss: 0.015990284919738768; (train.py:98)
[INFO] 2017-05-20T23:38:58+0800: Evaluation: (128, 372, 0.256) (train.py:102)
[INFO] 2017-05-20T23:40:55+0800: Epoch: 383; Train loss: 0.009654033170455812; Dev loss: 0.040710784912109374; (train.py:98)
[INFO] 2017-05-20T23:42:57+0800: Epoch: 384; Train loss: 0.0060445526011054; Dev loss: 0.008642628669738769; (train.py:98)
[INFO] 2017-05-20T23:44:19+0800: Evaluation: (109, 391, 0.218) (train.py:102)
[INFO] 2017-05-20T23:46:23+0800: Epoch: 385; Train loss: 0.011926524960098895; Dev loss: 0.0035115623474121093; (train.py:98)
[INFO] 2017-05-20T23:48:29+0800: Epoch: 386; Train loss: 0.007147182724120128; Dev loss: 0.0020287089347839357; (train.py:98)
[INFO] 2017-05-20T23:49:50+0800: Evaluation: (108, 392, 0.216) (train.py:102)
[INFO] 2017-05-20T23:51:58+0800: Epoch: 387; Train loss: 0.00807395258361425; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-20T23:54:00+0800: Epoch: 388; Train loss: 0.009193228370692575; Dev loss: 0.007155322074890137; (train.py:98)
[INFO] 2017-05-20T23:55:29+0800: Evaluation: (120, 380, 0.24) (train.py:102)
[INFO] 2017-05-20T23:57:27+0800: Epoch: 389; Train loss: 0.00820254630332447; Dev loss: 0.0018551597595214844; (train.py:98)
[INFO] 2017-05-20T23:59:24+0800: Epoch: 390; Train loss: 0.01402400930392943; Dev loss: 0.00832355499267578; (train.py:98)
[INFO] 2017-05-21T00:00:42+0800: Evaluation: (108, 392, 0.216) (train.py:102)
[INFO] 2017-05-21T00:02:38+0800: Epoch: 391; Train loss: 0.008234735509865015; Dev loss: 0.001619636058807373; (train.py:98)
[INFO] 2017-05-21T00:04:36+0800: Epoch: 392; Train loss: 0.01338587316607227; Dev loss: 0.011767932891845703; (train.py:98)
[INFO] 2017-05-21T00:06:05+0800: Evaluation: (140, 360, 0.28) (train.py:102)
[INFO] 2017-05-21T00:08:04+0800: Epoch: 393; Train loss: 0.012368220770118829; Dev loss: 0.00014000000059604645; (train.py:98)
[INFO] 2017-05-21T00:10:06+0800: Epoch: 394; Train loss: 0.015150885688089557; Dev loss: 0.0043460350036621095; (train.py:98)
[INFO] 2017-05-21T00:11:43+0800: Evaluation: (148, 352, 0.296) (train.py:102)
[INFO] 2017-05-21T00:13:50+0800: Epoch: 395; Train loss: 0.0059550420946276475; Dev loss: 0.0008689038157463074; (train.py:98)
[INFO] 2017-05-21T00:15:46+0800: Epoch: 396; Train loss: 0.011778306600244736; Dev loss: 0.003055060863494873; (train.py:98)
[INFO] 2017-05-21T00:17:16+0800: Evaluation: (123, 377, 0.246) (train.py:102)
[INFO] 2017-05-21T00:19:16+0800: Epoch: 397; Train loss: 0.0074012965490296775; Dev loss: 0.00804623031616211; (train.py:98)
[INFO] 2017-05-21T00:21:20+0800: Epoch: 398; Train loss: 0.006711124063594388; Dev loss: 0.0009129296541213989; (train.py:98)
[INFO] 2017-05-21T00:22:52+0800: Evaluation: (138, 362, 0.276) (train.py:102)
[INFO] 2017-05-21T00:24:55+0800: Epoch: 399; Train loss: 0.01025781763246872; Dev loss: 0.00014000000059604645; (train.py:98)
